import numpy as np
import gzip
import matplotlib.pyplot as plt
import time


def load_data(filename, num_images):
    with gzip.open(filename, 'rb') as f:
        data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)
    return data.reshape(num_images, 28, 28)


def load_labels(filename, num_labels):
    with gzip.open(filename, 'rb') as f:
        data = np.frombuffer(f.read(), dtype=np.uint8, offset=8)
    return data.reshape(num_labels, 1)

# Assign Files:
# x_train (Training Images: 60,000) , y_train (Training Labels: 60,000)
# x_test (Test Images: 10,000), y_test (Testing Labels: 10,000)


x_train = load_data('mnist/train-images-idx3-ubyte.gz', 60000)
y_train = load_labels('mnist/train-labels-idx1-ubyte.gz', 60000)
x_test = load_data('mnist/t10k-images-idx3-ubyte.gz', 10000)
y_test = load_labels('mnist/t10k-labels-idx1-ubyte.gz', 10000)

image_indices = np.arange(25)

# Verify Data is inputted correctly
fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(10, 10))

# Display each image in a subplot
for i, ax in enumerate(axes.flat):

    # Take 25 x_train values
    input_image = x_train[image_indices[i]]

    # Reshape the input to 28x28 Pixels
    input_image = np.reshape(input_image, (28, 28))

    # Use Grayscale
    ax.imshow(input_image, cmap='gray')

    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f'{y_train[image_indices[i]][0]}', fontsize=15)

# Adjust the spacing between the subplots
plt.tight_layout()

# Show the plot
plt.show()

# Reshape into 4D tensor
x_train = x_train.reshape((60000, 28, 28, 1))
x_test = x_test.reshape((10000, 28, 28, 1))

from keras.utils import to_categorical

# One Hot Encoding Format (Integer Labels to Binary Matrix)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Scale pixel values between 0 and 1
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Expand Dimensions
x_train = np.expand_dims(x_train, axis=-1)
x_test = np.expand_dims(x_test, axis=-1)


from keras import layers, models

# Define the model architecture
model = models.Sequential([
    # 2D Convolutional Layer to model w/ 32 filters with 3x3 Square Matrix
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),

    # Reduce Dimensionality of Output (Prevent Overfitting)
    layers.MaxPooling2D((2,2)),

    # Reshape output of previous layer to one dimensional vector
    layers.Flatten(),

    # Dense Layer with Activation Layer Softmax (0-1) (val < 0 small prob.  ; 0 < val < 1 large prob.)
    layers.Dense(10, activation='softmax')
])


from keras.optimizers import SGD

# Gradient Descent with Momentum
optimizer = SGD(learning_rate=0.001, momentum=0.9)

# Compile the model with the optimizer
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# Start Time to measure training time
start_time = time.time()

# Train the model Change Epoch for Iterations
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# End time when finished training
end_time = time.time()

# Calculate the training time (End - Start)
training_time = end_time - start_time

# Start time to measure testing time
start_time = time.time()

# Test with Labels and Images
test_loss, test_acc = model.evaluate(x_test, y_test)

# End time when finished testing
end_time = time.time()

# Calculate Testing Time (End - Start)
testing_time = end_time - start_time

# Print the test accuracy, train time, and test time
print('Test accuracy:', test_acc)
print("Training time: {:.2f} seconds".format(training_time))
print("Testing time: {:.2f} seconds".format(testing_time))

# Plot Training Loss and Validation Loss
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

# Plot training and validation accuracy
plt.figure()
plt.plot(history.history['accuracy'], label='Training accuracy')
plt.plot(history.history['val_accuracy'], label='Validation accuracy')
plt.legend()
plt.show()